{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize text\n",
    "# NLTK considers capital letters and small letters differently.\n",
    "# For example: Fox and fox are considered as two different words.\n",
    "# Hence, we convert all letters of our text into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Words\n",
    "\n",
    "#We split the text sentence/paragraph into a list of words. Each word in the list is called a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize text \n",
    "words = word_tokenize(text)\n",
    "print (words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#As you can see in the above output, there’s a token for full-stop (.).\n",
    "\n",
    "#It’s a punctuation mark. Generally, we get rid of such punctuation marks while analyzing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in words if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will remove stop words from our text data using the default stopwords corpus present in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of English Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the words list\n",
    "words_filtered = words[:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    if word in stop_words:        \n",
    "        words_filtered.remove(word)\n",
    "print (words_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating Stop Words Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, you don’t want to omit some stopwords for your text analysis. In such case, you have to remove those words from the stopwords list.\n",
    "\n",
    "Let’s suppose, you want the words over and under for your text analysis. The words “over” and “under” are present in the stopwords corpus by default. Let’s remove them from the stopwords corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'herself', 'is', 'wouldn', 'did', 'am', 'just', 'didn', 'both', 'with', 'which', 'until', 'you', 'hasn', 'because', 't', 'there', 'yours', 'mightn', 'hadn', 'theirs', 'are', 'same', 'him', 'll', 'can', 'having', 'has', 'off', \"needn't\", \"mightn't\", 'then', \"won't\", 'some', \"mustn't\", 'o', 'not', 'them', 'now', 'ourselves', \"shouldn't\", \"aren't\", 'a', 'against', 'was', 'ma', 'if', \"should've\", 're', 'should', 'ain', 'shouldn', \"didn't\", 'who', 'up', 'each', \"you'll\", 'himself', \"isn't\", 'as', 'into', 'had', 'down', 's', \"she's\", 'above', 'won', \"wasn't\", 'while', 'nor', 'be', 'any', 'only', 'do', 'it', 'own', 'd', 'i', \"you're\", 'out', 'other', 'why', 'we', 'were', 'aren', 'after', \"hadn't\", 'needn', 'me', 'when', 'again', 'he', 'so', \"don't\", 'its', 'by', 'in', 'and', 'wasn', 'ours', \"that'll\", 'or', 'about', 'than', 'our', 'from', 'through', 'being', 'too', 'but', 'below', 'these', 'will', 'm', 'the', 'isn', 'that', 'here', 'their', 'his', 'have', 'an', 'all', 'don', 'her', \"wouldn't\", 'yourselves', 'what', 'does', 'doing', 'once', 'myself', 'to', 'haven', \"shan't\", 'more', 'shan', 'on', \"you've\", 'at', 'no', \"hasn't\", 'my', \"haven't\", 'been', 'for', 'further', 'before', 'very', 'doesn', 'your', 'between', 'those', 'during', 'most', \"you'd\", 'yourself', 'this', 'weren', 'whom', 'how', 'couldn', 'hers', 'they', 'themselves', \"doesn't\", 'mustn', 'where', 'few', 've', \"weren't\", 'such', 'she', 'y', \"couldn't\", \"it's\", 'itself', 'of'}\n"
     ]
    }
   ],
   "source": [
    "# set() function removes duplicate entries from the list\n",
    "stop_words = set(stopwords.words('english')) - set(['over', 'under'])\n",
    " \n",
    "#stopwords_english = [str(item) for item in stop_words]\n",
    "#print (stopwords_english)\n",
    " \n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
